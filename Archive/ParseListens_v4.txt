import zipfile
import json
import pandas as pd
import os
import io
import time
import urllib.parse
import urllib.request
from tkinter import Tk, filedialog
from tqdm import tqdm  # progress bar
from datetime import datetime, UTC, timezone


"""
Parses and analyzes music listens exported as a zip from Listenbrainz. The exported zip file contains 3 items:

1. "user.json" - a json file with user name and ID; this is unused

2. "feedback.jsonl" - a json lines file with likes and dislikes, each as a distinct line
    * Example row from "feedback.jsonl":
        {"score": 1, "created": 1476644563, "recording_mbid": "25b22e5e-052c-4550-85c6-9c1a7efe5dba", "recording_msid": null}

3. "listens" folder:
    * Contains one subfolder per year (e.g. "2016", "2025") 
    * Each year subfolder contains a set of json lines files "1.jsonl", "12.jsonl", etc.
    * Each "*.jsonl" listen file contains listens in the form of single-line json objects
    * Example rows (listens) from a "*.jsonl" file within a year folder:
        {"inserted_at": 1764740348.909054, "listened_at": 1764740227, "track_metadata": {"track_name": "Etched Headplate", "artist_name": "Burial", "mbid_mapping": {"caa_id": 41334538769, "artists": [{"artist_mbid": "9ddce51c-2b75-4b3e-ac8c-1db09e7c89c6", "join_phrase": "", "artist_credit_name": "Burial"}], "artist_mbids": ["9ddce51c-2b75-4b3e-ac8c-1db09e7c89c6"], "release_mbid": "e08c3db9-fc33-4d4e-b8b7-818d34228bef", "recording_mbid": "1eacb3ca-e8e1-4588-920d-1187dcb8ca79", "recording_name": "Etched Headplate", "caa_release_mbid": "02aa03a5-001b-4e5a-b3ad-23ad0fadb49c"}, "release_name": "Untrue", "recording_msid": "2b7b424f-b5f5-4ef2-bd2d-e80834708f02", "additional_info": {"duration": 362, "origin_url": "https://music.youtube.com/playlist?list=OLAK5uy_l-q8XlDmU4d7d2dgjpZBYPC-wFFKQTKrA", "submission_client": "Web Scrobbler", "music_service_name": "YouTube Music", "submission_client_version": "3.18.0"}}}    
        {"inserted_at": 1762107306.802417, "listened_at": 1762107215, "track_metadata": {"track_name": "I Knew You Were Waiting (For Me)", "artist_name": "George Michael & Aretha Franklin", "mbid_mapping": {"caa_id": 15472428393, "artists": [{"artist_mbid": "ccb8f30e-4d71-40c4-8b1d-846dafe73e2c", "join_phrase": " & ", "artist_credit_name": "George Michael"}, {"artist_mbid": "2f9ecbed-27be-40e6-abca-6de49d50299e", "join_phrase": "", "artist_credit_name": "Aretha Franklin"}], "artist_mbids": ["ccb8f30e-4d71-40c4-8b1d-846dafe73e2c", "2f9ecbed-27be-40e6-abca-6de49d50299e"], "release_mbid": "69205df9-e4c3-41f2-9ff5-ad9714b0c210", "recording_mbid": "e5acf34e-d0eb-4e61-8269-b1d56d81d971", "recording_name": "I Knew You Were Waiting for Me", "caa_release_mbid": "bf240ed0-9e0c-3a44-ab3c-94dc69216af0"}, "release_name": "Aretha (Expanded Edition)", "recording_msid": "db4caef1-f087-4a35-9e79-51b14bffc882", "additional_info": {"duration_ms": 242000, "submission_client": "Pano Scrobbler", "submission_client_version": "4.13"}}}
        {'inserted_at': 1714272371.501321, 'listened_at': 1550646006, 'track_metadata': {'track_name': 'Easy on Me (feat. FATHERDUDE)', 'artist_name': 'Kill Paris', 'mbid_mapping': None, 'release_name': 'Galaxies Within Us', 'recording_msid': '058b6225-8311-4c7f-ad79-d34e1bc334bc', 'additional_info': {'submission_client': 'ListenBrainz lastfm importer', 'lastfm_artist_mbid': 'cfa44aeb-2cfa-40ff-b2bf-cef345312325', 'lastfm_release_mbid': '732b3d57-cdc5-401b-91d4-114b5e009f65'}}}
  

This module retrieves the following priority items from the listen objects.

1. Artist name(s), e.g. "Burial"
    * This is given by "artist_credit_name" from the list "artists" in "mbid_mapping" object if available
    * Fallback to simple "artist_name" from "track_metadata" if "mbid_mapping" is None
    * If a listen is tagged with multiple artists in "artists" list, count the listen towards the total for each artist separately

2. Album name, e.g. "Untrue" (given by "release_name" from the json object)

3. Track duration (given by "duration_ms", in milliseconds, or "duration", in s, from within "additional_info")

4. Recording_mbid for cross-linking likes with tracks


To Do:
I think I'd like to re-factor this be able to make a VERY minimalist interface to trigger "arbitrary" reports.
I'd like to wrap it in:
  1. A UI element (button) to select (and render an indication after selection) the zip file
  2. A set of entry boxes for the input variables into the reporting functions 
    * number of years
    * select by tracks/duration
    * Number for top n (count)
  3. A dropdown to select which function(s) to run (list Albums/artists/tracks)
  4. An "Analyze" button to do the analysis

"""

def select_zip_file():
    root = Tk()
    root.withdraw()
    file_path = filedialog.askopenfilename(
        title="Select ListenBrainz Export ZIP",
        filetypes=[("ZIP files", "*.zip")]
    )
    return file_path


def parse_listenbrainz_zip(zip_path):
    with zipfile.ZipFile(zip_path, 'r') as z:
        # Load user.json
        user_info = json.loads(z.read("user.json").decode("utf-8"))
        
        # Load feedback.jsonl
        feedback = []
        if "feedback.jsonl" in z.namelist():
            with z.open("feedback.jsonl") as f:
                for line in f:
                    feedback.append(json.loads(line.decode("utf-8")))
        
        # Load listens
        listens = []
        for name in z.namelist():
            if name.startswith("listens/") and name.endswith(".jsonl"):
                with z.open(name) as f:
                    for line in f:
                        listens.append(json.loads(line.decode("utf-8")))
    
    return user_info, feedback, listens


def load_genre_cache(cache_path):
    if os.path.exists(cache_path):
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}


def save_genre_cache(cache, cache_path):
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)


def normalize_listens(listens, zip_path=None):
    records = []
    
    # Prepare log file if zip_path is provided
    log_file = None
    if zip_path:
        base_dir = os.path.dirname(zip_path)
        reports_dir = os.path.join(base_dir, "reports")
        os.makedirs(reports_dir, exist_ok=True)
        log_file = os.path.join(reports_dir, "missing_album_info.txt")
    
    # Do the processing
    for idx, l in enumerate(listens):
        meta = l.get("track_metadata", {})
        
        # mbid_mapping may be None
        mbid_mapping = meta.get("mbid_mapping") or {}
        artists = []
        if "artists" in mbid_mapping and mbid_mapping["artists"]:
            # Prefer artist_credit_name from mbid_mapping
            artists = [a.get("artist_credit_name") for a in mbid_mapping["artists"] if a.get("artist_credit_name")]
        else:
            # Fallback to artist_name
            if meta.get("artist_name"):
                artists = [meta["artist_name"]]
            else:
                print(f"[WARN] Missing artist info- Track='{meta.get('track_name','Unknown')}', Album='{meta.get('release_name','Unknown')}'")
                artists = ["Unknown"]
        
        album_name = meta.get("release_name", "Unknown")
        
        if album_name == "Unknown":
            warning = f"[WARN] Unknown album- Artist='{artists[0]}', Track='{meta.get('track_name','Unknown')}', Album='Unknown'"
            if log_file:
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(warning + "\n")
        
        # Duration: prefer ms, fallback to s
        info = meta.get("additional_info", {}) or {}
        duration_ms = info.get("duration_ms")
        if duration_ms is None and "duration" in info:
            duration_ms = info["duration"] * 1000
        
        listened_at = l.get("listened_at")
        listened_dt = datetime.fromtimestamp(listened_at, UTC) if listened_at else None
        
        recording_mbid = None
        if "mbid_mapping" in meta and meta["mbid_mapping"]:
            recording_mbid = meta["mbid_mapping"].get("recording_mbid")
        elif "additional_info" in meta and meta["additional_info"].get("lastfm_recording_mbid"):
            recording_mbid = meta["additional_info"]["lastfm_recording_mbid"]

        # Create one record per artist
        for artist in artists:
            records.append({
                "artist": artist,
                "album": album_name,
                "duration_ms": duration_ms or 0,
                "listened_at": listened_dt,
                "recording_mbid": recording_mbid
            })

    
    return pd.DataFrame(records)


def load_feedback(feedback):
    """Extract liked recording MBIDs from feedback list"""
    likes = set()
    for row in feedback:
        if row.get("score") == 1 and row.get("recording_mbid"):
            likes.add(row["recording_mbid"])
    return likes


def report_artists_with_likes(df, feedback):
    liked_mbids = load_feedback(feedback)

    # Filter listens that were liked
    liked_listens = df[df["recording_mbid"].isin(liked_mbids)]

    # Unique liked recordings per artist
    unique_likes = (
        liked_listens.groupby("artist")["recording_mbid"]
        .nunique()
        .reset_index(name="unique_likes")
    )

    # Total liked listens per artist
    total_liked_listens = (
        liked_listens.groupby("artist")
        .size()
        .reset_index(name="total_liked_listens")
    )

    # Merge both counts
    artist_likes = pd.merge(unique_likes, total_liked_listens, on="artist")

    # Sort by unique likes (main sorting variable)
    artist_likes = artist_likes.sort_values(
        ["unique_likes", "total_liked_listens"], ascending=[False, False]
    )

    return artist_likes




def report_artists_threshold(df, mins=30, tracks=15):
    grouped = df.groupby("artist").agg(
        total_tracks=("artist", "count"),
        total_duration_ms=("duration_ms", "sum")
    )
    grouped["total_duration_hours"] = (grouped["total_duration_ms"] / (1000 * 60 * 60)).round(1)
    
    # Filter by total listening time or total tracks listened
    filtered = grouped[(grouped.total_tracks > tracks) | (grouped.total_duration_ms > mins*60*1000)]
    return filtered.sort_values("total_tracks", ascending=False)[["total_tracks","total_duration_hours"]]


def report_top_artists(df, years=None, by="duration_ms", topn=100):
    if years:
        cutoff = datetime.now(timezone.utc).year - years
        df = df[df.listened_at.dt.year >= cutoff]
    grouped = df.groupby("artist").agg(
        total_tracks=("artist", "count"),
        total_duration_ms=("duration_ms", "sum")
    )
    
    grouped["total_duration_hours"] = (grouped["total_duration_ms"] / (1000 * 60 * 60)).round(1)
    
    if by == "duration_ms":
        key = "total_duration_hours"
    else:
        key = "total_tracks"
    return grouped.sort_values(key, ascending=False).head(topn)[["total_tracks","total_duration_hours"]]


def report_top_albums(df, years=None, by="duration_ms", topn=100):
    if years:
        cutoff = datetime.now(timezone.utc).year - years
        df = df[df.listened_at.dt.year >= cutoff]
    grouped = df.groupby("album").agg(
        total_tracks=("album", "count"),
        total_duration_ms=("duration_ms", "sum")
    )
    grouped["total_duration_hours"] = (grouped["total_duration_ms"] / (1000 * 60 * 60)).round(1)
    
    if by == "duration_ms":
        key = "total_duration_hours"
    else:
        key = "total_tracks"
    return grouped.sort_values(key, ascending=False).head(topn)[["total_tracks","total_duration_hours"]]


def save_report(df, report_name, zip_path):
    # Create a "reports" folder next to the zip file
    base_dir = os.path.dirname(zip_path)
    reports_dir = os.path.join(base_dir, "reports")
    os.makedirs(reports_dir, exist_ok=True)

    timestamp = time.strftime("%Y%m%d-%H%M%S")
    filename = f"{report_name}_{timestamp}.txt"
    filepath = os.path.join(reports_dir, filename)
    
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"=== {report_name} ===\n\n")
        f.write(df.to_string(index=False))
        
    print(f"Report saved to {filepath}")


def get_artist_genres(artist_name):
    # Build the MusicBrainz query URL
    query = urllib.parse.quote(artist_name)
    url = f"https://musicbrainz.org/ws/2/artist/?query={query}&fmt=json"
    
    try:
        with urllib.request.urlopen(url) as resp:
            data = json.load(resp)
            if "artists" in data and data["artists"]:
                artist = data["artists"][0]
                tags = artist.get("tags", [])
                return [t["name"] for t in tags] if tags else ["Unknown"]
    except Exception as e:
        print(f"[WARN] Genre lookup failed for '{artist_name}': {e}")   
    return ["Unknown"]


def enrich_report_with_genres(report_df, zip_path, outfile="Artists_In_Library"):
    import os
    base_dir = os.path.dirname(zip_path)
    reports_dir = os.path.join(base_dir, "reports")
    os.makedirs(reports_dir, exist_ok=True)
    
    # Load or initialize cache
    cache_path = os.path.join(reports_dir, "genres_cache.json")
    genre_cache = load_genre_cache(cache_path)
    
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    filename = f"{outfile}_{timestamp}.csv"
    out_path = os.path.join(reports_dir, filename)

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("Artist,Tracks,Hours,Genres\n")

        genres = []
        recent = []  # keep track of last few lookups

        with tqdm(total=len(report_df), desc="Enriching artists") as pbar:
            for artist in report_df.index:                
                # Check cache first
                if artist in genre_cache:
                    g = genre_cache[artist]
                else:
                    g = get_artist_genres(artist)
                    genre_cache[artist] = g
                    save_genre_cache(genre_cache, cache_path)
                    time.sleep(1.2)  # only sleep when querying API

                
                genre_str = ", ".join(g)
                genres.append(genre_str)

                # Write row immediately
                tracks = report_df.loc[artist, "total_tracks"]
                hours = report_df.loc[artist, "total_duration_hours"]
                f.write(f"{artist},{tracks},{hours:.2f},{genre_str}\n")
                f.flush()

                # Update rolling summary (last 3 artists)
                recent.append(f"{artist}: {genre_str}")
                if len(recent) > 3:
                    recent.pop(0)
                pbar.set_postfix_str(" \n ".join(recent))  # was a | instead of \n

                pbar.update(1)

    enriched = report_df.copy()
    enriched["Genres"] = genres
    return enriched


if __name__ == "__main__":
    zip_path = select_zip_file()
    user_info, feedback, listens = parse_listenbrainz_zip(zip_path)
    df = normalize_listens(listens, zip_path)
    
    # Save basic reports
    save_report(report_top_artists(df, years=2, by="tracks"), "Top100_Artists_Last2Years_Listens", zip_path)
    # save_report(report_top_albums(df, years=2, by="duration_ms"), "Top100_Albums_Last2Years_Duration", zip_path)
    save_report(report_top_albums(df, years=2, by="tracks"), "Top100_Albums_Last2Years_Listens", zip_path)
    save_report(report_top_albums(df, years=None, by="tracks"), "Top100_Albums_AllTime_Listens", zip_path)
    # save_report(report_top_albums(df, years=None, by="duration_ms"), "Top100_Albums_AllTime_Duration", zip_path)

    artist_likes_report = report_artists_with_likes(df, feedback)
    save_report(artist_likes_report, "Artists_With_Likes", zip_path)

    # Save a report of all artists "in library"
    # Temporarily disable to avoid waiting for data pull
    artist_report = report_artists_threshold(df, mins=30, tracks=15)
    enriched_report = enrich_report_with_genres(artist_report, zip_path)
